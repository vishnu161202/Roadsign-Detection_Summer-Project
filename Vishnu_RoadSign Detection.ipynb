{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e2f292a",
   "metadata": {},
   "source": [
    "# Road-Sign-Detection\n",
    "## Problem statement\n",
    "This dataset contains 877 images of 4 distinct classes for the objective of road sign detection.\n",
    "Bounding box annotations are provided in the PASCAL VOC format\n",
    "The classes are:Trafic Light;Stop;Speedlimit;Crosswalk.\n",
    "## To Predict\n",
    "Road Sign Detection using Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f20c30f",
   "metadata": {},
   "source": [
    "## Step 0: Import all the necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "89dc87ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import xml.etree.ElementTree as ET\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e9d1ae",
   "metadata": {},
   "source": [
    "## Step 1: Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "35b16e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = 'C:\\\\Users\\\\Vishnu\\\\Downloads\\\\archive\\\\images'\n",
    "annotation_folder = 'C:\\\\Users\\\\Vishnu\\\\Downloads\\\\archive\\\\annotations'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0b5a238c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_filenames = os.listdir(image_folder)\n",
    "annotations_filenames = os.listdir(annotation_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2032bca9",
   "metadata": {},
   "source": [
    "These lines list all the filenames in the image and annotation folders using os.listdir()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b5e0f4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty lists to store the data\n",
    "images = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8aff995",
   "metadata": {},
   "source": [
    "## Step 2: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a317af40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each image\n",
    "for image_filename in image_filenames:\n",
    "    # Load the image\n",
    "    image = cv2.imread(os.path.join(image_folder, image_filename))\n",
    "    \n",
    "    # Define the desired width and height for resizing\n",
    "    desired_width = 224\n",
    "    desired_height = 224\n",
    "    \n",
    "    # Resize the image to a consistent shape\n",
    "    image = cv2.resize(image, (desired_width, desired_height))\n",
    "    \n",
    "    images.append(image)\n",
    "\n",
    "    # Parse the corresponding XML annotation\n",
    "    annotation_filename = image_filename.replace('.png', '.xml')\n",
    "    annotation_path = os.path.join(annotation_folder, annotation_filename)\n",
    "    tree = ET.parse(annotation_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Extract bounding box coordinates and class label\n",
    "    xmin = int(root.find('.//xmin').text)\n",
    "    ymin = int(root.find('.//ymin').text)\n",
    "    xmax = int(root.find('.//xmax').text)\n",
    "    ymax = int(root.find('.//ymax').text)\n",
    "    class_label = root.find('.//name').text\n",
    "\n",
    "    labels.append(class_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d0a8d9",
   "metadata": {},
   "source": [
    "* This loop iterates through each image filename in the image_filenames list.\n",
    "* It reads each image using cv2.imread() and resizes it to the desired width and height using cv2.resize().\n",
    "* The resized image is appended to the images list.\n",
    "* The corresponding XML annotation filename is obtained by replacing the extension of the image filename with '.xml'.\n",
    "* The XML annotation file is parsed using xml.etree.ElementTree and the root element is obtained.\n",
    "* The bounding box coordinates (xmin, ymin, xmax, ymax) and class label are extracted from the XML.\n",
    "* The class label is appended to the labels list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "093a6bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert images and labels to numpy arrays\n",
    "images = np.stack(images)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e97dc417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map string labels to integer values\n",
    "label_mapping = {label: i for i, label in enumerate(np.unique(labels))}\n",
    "labels_mapped = np.array([label_mapping[label] for label in labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a96d46",
   "metadata": {},
   "source": [
    "These lines create a label mapping dictionary that maps unique class labels to integer values.\n",
    "The labels numpy array is mapped to its corresponding integer values using list comprehension.\n",
    "The labels_mapped array contains the mapped integer labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4381a938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform one-hot encoding of the labels\n",
    "num_classes = len(np.unique(labels_mapped))\n",
    "labels_encoded = to_categorical(labels_mapped, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea34ccd",
   "metadata": {},
   "source": [
    "The num_classes variable is set to the number of unique mapped labels.\n",
    "The labels_mapped array is one-hot encoded using to_categorical() from tensorflow.keras.utils."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "526947f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f99fec",
   "metadata": {},
   "source": [
    "The dataset is split into training and testing sets using train_test_split() from sklearn.model_selection.\n",
    "The images and labels_encoded arrays are split into X_train, X_test, y_train, and y_test with a test size of 20% and a random state of 42."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d240ab63",
   "metadata": {},
   "source": [
    "## Step 3: Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3b89fc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model architecture\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(desired_width, desired_height, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d21c5b5",
   "metadata": {},
   "source": [
    "A sequential model is created using Sequential() from tensorflow.keras.models.\n",
    "The model architecture consists of a 2D convolutional layer, max pooling layer, flatten layer, and two dense (fully connected) layers.\n",
    "The first convolutional layer has 32 filters, a kernel size of (3, 3), and uses the ReLU activation function.\n",
    "The max pooling layer has a pool size of (2, 2).\n",
    "The flatten layer flattens the output from the previous layer.\n",
    "The first dense layer has 128 units with the ReLU activation function.\n",
    "The final dense layer has the number of units equal to the number of classes and uses the softmax activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cca121",
   "metadata": {},
   "source": [
    "## Step 4: Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d2b427e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "22/22 [==============================] - 11s 411ms/step - loss: 4930.6401 - accuracy: 0.6120\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - 9s 390ms/step - loss: 292.9170 - accuracy: 0.7161\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - 9s 406ms/step - loss: 13.9364 - accuracy: 0.8859\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - 9s 388ms/step - loss: 3.8224 - accuracy: 0.9230\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - 9s 403ms/step - loss: 0.9695 - accuracy: 0.9857\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - 9s 389ms/step - loss: 0.1706 - accuracy: 0.9943\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - 9s 398ms/step - loss: 0.1941 - accuracy: 0.9957\n",
      "Epoch 8/10\n",
      "22/22 [==============================] - 9s 395ms/step - loss: 0.1543 - accuracy: 0.9872\n",
      "Epoch 9/10\n",
      "22/22 [==============================] - 9s 392ms/step - loss: 1.8212 - accuracy: 0.9558\n",
      "Epoch 10/10\n",
      "22/22 [==============================] - 9s 401ms/step - loss: 0.4914 - accuracy: 0.9886\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x277285c3730>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45896f7",
   "metadata": {},
   "source": [
    "The model is compiled with the categorical cross-entropy loss function, the Adam optimizer, and accuracy as the metric.\n",
    "The training data (X_train and y_train) is used to train the model using fit() function.\n",
    "The training is performed for 10 epochs with a batch size of 32."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5cdb06",
   "metadata": {},
   "source": [
    "## Step 5: Prediction and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d344e62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 1s 93ms/step - loss: 36.9415 - accuracy: 0.7670\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[36.941471099853516, 0.7670454382896423]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2b4105c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 1s 93ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test)\n",
    "predicted_classes = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f7fa6658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert predicted classes to original label format\n",
    "predicted_labels = np.array([np.unique(labels)[pred_class] for pred_class in predicted_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "eaae1c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with image filename, predicted class, and original class\n",
    "results_df = pd.DataFrame({\n",
    "    'Image Filename': image_filenames[:len(predicted_labels)],\n",
    "    'Predicted Class': predicted_labels,\n",
    "    'Original Class': np.array([np.unique(labels)[np.argmax(label)] for label in y_test])\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "254a8cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "results_df.to_csv('test_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dd314d",
   "metadata": {},
   "source": [
    "The model is used to make predictions on the testing data (X_test) using predict().\n",
    "The predicted classes are obtained by finding the indices of the highest probability values using np.argmax().\n",
    "The predicted classes are then converted back to their original label format using the label mapping dictionary.\n",
    "A DataFrame (results_df) is created to store the image filename, predicted class, and original class for each test image.\n",
    "The DataFrame is saved to a CSV file named 'test_results.csv' using to_csv()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5f5dd692",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = pd.read_csv(\"test_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dd022863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image Filename</th>\n",
       "      <th>Predicted Class</th>\n",
       "      <th>Original Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>road0.png</td>\n",
       "      <td>speedlimit</td>\n",
       "      <td>speedlimit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>road1.png</td>\n",
       "      <td>trafficlight</td>\n",
       "      <td>stop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>road10.png</td>\n",
       "      <td>speedlimit</td>\n",
       "      <td>crosswalk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>road100.png</td>\n",
       "      <td>speedlimit</td>\n",
       "      <td>stop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>road101.png</td>\n",
       "      <td>speedlimit</td>\n",
       "      <td>speedlimit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Image Filename Predicted Class Original Class\n",
       "0      road0.png      speedlimit     speedlimit\n",
       "1      road1.png    trafficlight           stop\n",
       "2     road10.png      speedlimit      crosswalk\n",
       "3    road100.png      speedlimit           stop\n",
       "4    road101.png      speedlimit     speedlimit"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6253763a",
   "metadata": {},
   "source": [
    "## Saving Model and Taking Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3d36f652",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Vishnu\\Downloads\\Road-sign\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Vishnu\\Downloads\\Road-sign\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('C:\\\\Users\\\\Vishnu\\\\Downloads\\\\Road-sign')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9476d2b0",
   "metadata": {},
   "source": [
    "Saving the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "15b7a1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model('C:\\\\Users\\\\Vishnu\\\\Downloads\\\\Road-sign')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9865c6",
   "metadata": {},
   "source": [
    "Loading the trained model for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ecb43a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = cv2.imread('C:\\\\Users\\\\Vishnu\\\\Downloads\\\\archive\\\\images\\\\road0.png')\n",
    "image_size = (224, 224)\n",
    "preprocessed_image = cv2.resize(input_image, image_size) / 255.0\n",
    "preprocessed_image = np.expand_dims(preprocessed_image, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6602a67f",
   "metadata": {},
   "source": [
    " Loading and preprocessing the input image for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1eb88583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 68ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = loaded_model.predict(preprocessed_image)\n",
    "predicted_label_index = np.argmax(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb83dcc5",
   "metadata": {},
   "source": [
    "Making predictions on the input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ade7179d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = ['Traffic Light', 'Stop', 'Speed Limit', 'Crosswalk']\n",
    "predicted_label = class_labels[predicted_label_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50bd15c",
   "metadata": {},
   "source": [
    "Mapping the predicted label index to the corresponding class label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4876751",
   "metadata": {},
   "source": [
    "### Print the predicted label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bb2878c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label: Crosswalk\n"
     ]
    }
   ],
   "source": [
    "print('Predicted Label:', predicted_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d2fccb",
   "metadata": {},
   "source": [
    "## Accuracy of the Model: 0.7670454382896423"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
